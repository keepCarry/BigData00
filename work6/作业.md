# 题1

在`SqlBase.g4`文件添加关键词并用自带的antlr4工具进行编译

```
statement
    | SHOW VERSION                     #showJavaSparkVersion

ansiNonReserved
    | VERSION

nonReserved
    | VERSION

VERSION: 'VERSION' | 'VERSIONS';
```

查看`SqlBaseParser`，新增了`ShowJavaSparkVersionContext`

```scala
public static class ShowJavaSparkVersionContext extends StatementContext {
    public TerminalNode SHOW() { return getToken(SqlBaseParser.SHOW, 0); }
    public TerminalNode VERSION() { return getToken(SqlBaseParser.VERSION, 0); }
    public ShowJavaSparkVersionContext(StatementContext ctx) { copyFrom(ctx); }
    @Override
    public void enterRule(ParseTreeListener listener) {
        if ( listener instanceof SqlBaseListener ) ((SqlBaseListener)listener).enterShowJavaSparkVersion(this);
    }
    @Override
    public void exitRule(ParseTreeListener listener) {
        if ( listener instanceof SqlBaseListener ) ((SqlBaseListener)listener).exitShowJavaSparkVersion(this);
    }
    @Override
    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
        if ( visitor instanceof SqlBaseVisitor ) return ((SqlBaseVisitor<? extends T>)visitor).visitShowJavaSparkVersion(this);
        else return visitor.visitChildren(this);
    }
}
```

查看`SqlBaseBaseVisitor`，新增了`visitShowJavaSparkVersion`方法

```scala
/**
* {@inheritDoc}
*
* <p>The default implementation returns the result of calling
* {@link #visitChildren} on {@code ctx}.</p>
*/
@Override public T visitShowJavaSparkVersion(SqlBaseParser.ShowJavaSparkVersionContext ctx) { return visitChildren(ctx); }
```

在`SparkSqlParser`中重写`visitShowJavaSparkVersion`并实现自定义command

```scala
SparkSqlParser
override def visitShowJavaSparkVersion(
    ctx: ShowJavaSparkVersionContext): LogicalPlan = withOrigin(ctx) {
ShowJavaSparkVersionCommand()
}
ShowJavaSparkVersionCommand
case class ShowJavaSparkVersionCommand() extends RunnableCommand {

  override val output: Seq[Attribute] =
    Seq(AttributeReference("version", StringType, nullable = true)())

  override def run(sparkSession: SparkSession): Seq[Row] = {

    val java_version = System.getProperty("java.version");
    val spark_version = sparkSession.sparkContext.version
    val outputString_java = s"java.version:${java_version}"
    val outputString_spark = s"spark.version:${spark_version}"
    Seq(Row(outputString_java), Row(outputString_spark))
  }
}
```

# 题2

满足CombineFilters、CollapseProject、BooleanSimplification优化规则

```sql
select name from (select name, age from people where age>15 and !(age is null)) t where t.age>12
```



满足ConstantFolding、PushDownPredicates、ReplaceDistinctWithAggregate、ReplaceExceptWithAntiJoin、FoldablePropagation优化规则

```sql
select distinct name, length('people') as size from 
(select name, age from people where age>15) t where t.age>12 
except select * from people where age is null 
order by size
```



# 题3

合并同一个属性值存在多个大于等情况,例如：age > 10 and age > 12 合并成 age > 10   【**参考了其他的代码**】

```scala
package org.apache.spark.examples.sql

import org.apache.spark.sql.SparkSession

import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.plans.logical.{Filter, LogicalPlan}
import org.apache.spark.sql.catalyst.rules.Rule

object SparkSQLExample {

  def main(args: Array[String]): Unit = {
    // $example on:init_session$
    val spark = SparkSession
      .builder()
      .appName("Spark SQL basic example")
      .config("spark.some.config.option", "some-value")
      .config("spark.sql.planChangeLog.level", "warn")
      .withExtensions(extensions => {
        extensions.injectOptimizerRule{
          session => {
            MyPushDown(session)
          }
        }
      })
      .getOrCreate()

    import spark.sql
    val df = spark.read.json("examples/src/main/resources/people.json")
    df.createOrReplaceTempView("people")

    sql("select distinct name as pname " +
      "from (select name, age from people " +
      "where age>10) t " +
      "where t.age>12" ).show()

    spark.stop()
  }
}

case class MyPushDown(session: SparkSession) extends Rule[LogicalPlan] with PredicateHelper {
  def apply(plan: LogicalPlan): LogicalPlan = plan transformDown {
    case Filter(fc, child) =>
      val expressions = ExpressionSet(splitConjunctivePredicates(fc))
      val greaterThan = expressions.filter(_.isInstanceOf[GreaterThan])
        .groupBy(_.asInstanceOf[GreaterThan].left.asInstanceOf[Attribute].name).toArray
        .map{
        item =>
          item._2.minBy(_.asInstanceOf[GreaterThan].
            right.asInstanceOf[Literal].value.toString).asInstanceOf[GreaterThan]
      }
      val result_expressions = expressions.filter{
        case expression: GreaterThan => false
        case _ => true
      } ++ greaterThan
      result_expressions.reduceOption(And) match {
        case Some(ac) =>
          Filter(ac, child)
        case None =>
          Filter(fc, child)
      }
  }
}
```

